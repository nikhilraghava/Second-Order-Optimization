{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's Method - Root Finding\n",
    "\n",
    "Gradient descent is a very popular first order optimization technique that has five main steps,\n",
    "\n",
    "- Define a model\n",
    "- Define an error\n",
    "- Compute a gradient using our error\n",
    "- Update our weights using our gradient\n",
    "- Repeat the computations until gradient equals to zero\n",
    "\n",
    "First order optimization techniques are usually less computationally expensive to compute and less time expensive, converging pretty fast on large datasets. Second order optimization techniques on the other hand are faster when the second derivative is known and easy to compute. But the second derivative is often intractable to compute, requiring lots of computation. For certain problems, gradient descent can get stuck along paths of slow convergence around saddle points, whereas second order methods wont't.\n",
    "\n",
    "Trying out different optimization techniques for your specific problem is the best way to see what works best. Note: first order optimization uses the first derivative while the second order optimization uses the second derivative.\n",
    "\n",
    "In this notebook, we will be looking at Newton's method - root finding to perform second order optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Functions and Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pow\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 6*pow(x, 5)-5*pow(x, 4)-4*pow(x, 3)+3*pow(x, 2)\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    return 30*pow(x, 4)-20*pow(x, 3)-12*pow(x, 2)+6*x\n",
    "\n",
    "\n",
    "def dx(x):\n",
    "    return abs(0-f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function is a polynomial equation, $f(x) = 6x^5 - 5x^4 - 4x^3 + 3x^2$. The derivative of this function is, $f'(x) = 30x^4 - 20x^3 - 12x^2 + 6x$ If we were to plot our *f(x)* against *x*, we would get the following graph.\n",
    "\n",
    "![Imgur](https://i.imgur.com/DcYXVTW.png)\n",
    "<br>\n",
    "\n",
    "If we were to look at our graph, we can see that our roots are -0.8, 0, 0.6ish and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons_method(df, x0, e, print_res=False):\n",
    "    delta = dx(x0)\n",
    "    while delta > e:\n",
    "        x0 = x0 - f(x0)/df(x0)\n",
    "        delta = dx(x0)\n",
    "    if print_res:\n",
    "        print('Root is at: {}'.format(x0))\n",
    "        print('f(x) at root is: {}'.format(f(x0)))\n",
    "    return x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numerical analysis, Newton's method (also known as the Newton–Raphson method), named after Isaac Newton and Joseph Raphson, is a method for finding successively better approximations to the roots (or zeroes) of a real-valued function. It is one example of a root-finding algorithm, $x:f(x) = 0$. The Newton–Raphson method in one variable is implemented as follows:\n",
    "\n",
    "The method starts with a function $f$ defined over the real numbers $x$, the function's derivative $f′$, and an initial guess $x_0$ for a root of the function $f$. If the function satisfies the assumptions made in the derivation of the formula and the initial guess is close, then a better approximation $x_1$ is $x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$.\n",
    "\n",
    "Geometrically, $(x_1, 0)$ is the intersection of the $x$-axis and the tangent of the graph of $f$ at $(x_0, f(x_0))$. The process is repeated as $x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$ until a sufficiently accurate value is reached.\n",
    "\n",
    "The idea of the method is as follows: one starts with an initial guess which is reasonably close to the true root, then the function is approximated by its tangent line (which can be computed using the tools of calculus), and one computes the $x$-intercept of this tangent line (which is easily done with elementary algebra). This $x$-intercept will typically be a better approximation to the function's root than the original guess, and the method can be iterated.\n",
    "\n",
    "<br>\n",
    "![Newton's method](https://media.giphy.com/media/4T08sI1RuCgK8T9SB1/giphy.gif)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root is at: -0.7953336454431276\n",
      "f(x) at root is: 2.220446049250313e-16\n",
      "Root is at: 0\n",
      "f(x) at root is: 0.0\n",
      "Root is at: 0.6286669787778999\n",
      "f(x) at root is: -1.8043344596208044e-12\n",
      "Root is at: 1.0000000000002292\n",
      "f(x) at root is: 9.166001291305292e-13\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Run test - guessed roots\n",
    "    x0s = [-1, 0, 0.5, 2]\n",
    "\n",
    "    for x0 in x0s:\n",
    "        newtons_method(df, x0, 1e-10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing our results with our graph, we can see that the Newton's method has been successful in helping us find the roots of the equation. Computationally and visually, our roots are at -0.7953336454431276, 0, 0.6286669787778999 and 1.0000000000002292."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
